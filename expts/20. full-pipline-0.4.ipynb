{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.cluster import MeanShift\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "from skimage.transform import rotate\n",
    "from skimage.color import rgb2gray\n",
    "from deskew import determine_skew\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "cv2.setRNGSeed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_img(image,size=(1200,1600), n_iterations=1, return_inter=False):\n",
    "    def deskew(image):\n",
    "        grayscale = rgb2gray(image)\n",
    "        angle = np.round(determine_skew(grayscale),2)\n",
    "        if angle < 10 and angle > -10:\n",
    "            return image\n",
    "        rotated = (rotate(image, angle, resize=True) * 255).astype(np.uint8)\n",
    "        return rotated\n",
    "    # threshold\n",
    "    image = deskew(image)\n",
    "    image = cv2.resize(image,size,interpolation = cv2.INTER_LINEAR)\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,129,4)\n",
    "    #erosion\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    erosion = ~thresh.copy()\n",
    "    for i in range(n_iterations):\n",
    "        erosion = cv2.erode(erosion,kernel,iterations = i)\n",
    "    if return_inter:\n",
    "        return [image, thresh, erosion]\n",
    "    return erosion, image\n",
    "\n",
    "    # image = cv2.resize(image,size,interpolation = cv2.INTER_LINEAR)\n",
    "    # img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # thresh = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,129,4)\n",
    "    # kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "    # opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    # result = 255 - opening\n",
    "    # if return_inter:\n",
    "    #     return [image, opening, result]\n",
    "    # return result, image\n",
    "\n",
    "#generate_stats components into a dataframe\n",
    "def generate_stats(totalLabels,stats, centroids):\n",
    "    '''\n",
    "    totalLabels,stats, centroids : cv2 connectedComponentsWithStats outputs\n",
    "    '''\n",
    "    columns = ['label_id', 'left','top','width','height','area','centroid_x','centroid_y']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df['label_id'] = [i for i in range(1,totalLabels+1)]\n",
    "    df['left'] = stats[:,0]\n",
    "    df['top'] = stats[:,1]\n",
    "    df['width'] = stats[:,2]\n",
    "    df['height'] = stats[:,3]\n",
    "    df['area'] = stats[:,4]\n",
    "    df['centroid_x'] = centroids[:,0]\n",
    "    df['centroid_y'] = centroids[:,1]\n",
    "    return df\n",
    "\n",
    "#filter \n",
    "def filter_cv2_components(df, min_ar_thresh=1, max_ar_thresh=10,pixel_row_gap = 10):\n",
    "    '''\n",
    "    df : dataframe,\n",
    "    min_ar_thresh : min aspect ratio threshold,\n",
    "    max_ar_thresh : max aspect ratio threshold,\n",
    "    pixel_row_gap : min gap b/w pixels to consider them in difft rows\n",
    "    '''\n",
    "\n",
    "    df['aspect_ratio'] = np.round(df['width']/df['height'],2)\n",
    "    df = df.sort_values(by=['top', 'left']).reset_index(drop=True) #sort-by occurence\n",
    "\n",
    "    #if it's not a word component, do not use it.\n",
    "    # h_median =df['height'].median()\n",
    "    # df = df[df['height'] > h_median].reset_index(drop=True)\n",
    "    \n",
    "    #calculate rowID, colID\n",
    "    df['rowID'] = 1\n",
    "    row_id = 1\n",
    "    prev_val = df.loc[0, 'top']\n",
    "    for idx in range(1, df.shape[0]):\n",
    "        curr_val = df.loc[idx, 'top']\n",
    "        if curr_val - prev_val > pixel_row_gap:\n",
    "            row_id += 1\n",
    "        prev_val = curr_val\n",
    "        df.at[idx, 'rowID'] = int(row_id)\n",
    "    df['columnID'] = df.groupby('rowID')['left'].rank().astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_lexicon(label_id, df, k=5):\n",
    "    '''\n",
    "    label_id : word component id, \n",
    "    df : dataframe\n",
    "    k : #neighbors to build context\n",
    "    '''\n",
    "    if label_id not in df['label_id'].values:\n",
    "        print(\"unknown component\")\n",
    "        return\n",
    "    \n",
    "    #filter by current component and extract candidate neighbors\n",
    "    x,y,w,h,r,c = df[df['label_id'] == label_id][['centroid_x','centroid_y','width','height','rowID','columnID']].values[0]\n",
    "    r1 = (df['rowID'] >= r-1)\n",
    "    r2 = (df['rowID'] <= r+1)    \n",
    "    temp_df = df[r1 & r2]\n",
    "    temp_df = temp_df[temp_df['label_id'] != label_id]\n",
    "    \n",
    "    #rotation-invariance - TBD during preprocessing- deskew at start\n",
    "    \n",
    "    #scale-invariance\n",
    "    temp_df['new_centroid_x'] = (temp_df['centroid_x']-x)/w\n",
    "    temp_df['new_centroid_y'] = (y-temp_df['centroid_y'])/h\n",
    "    temp_df['new_left'] = (temp_df['left']-x)/w\n",
    "    temp_df['new_top'] = (y-temp_df['top'])/h\n",
    "    temp_df['new_width'] = temp_df['width']/w\n",
    "    temp_df['new_height'] = temp_df['height']/h\n",
    "    \n",
    "    #calculate distance & angle\n",
    "    temp_df['Euclidean'] = np.sqrt(temp_df['new_centroid_y']**2 + temp_df['new_centroid_x']**2)\n",
    "    temp_df['theta'] = np.degrees(np.arctan2(temp_df['new_centroid_y'], temp_df['new_centroid_x']))\n",
    "    temp_df['theta'] = (temp_df['theta'] + 360) % 360 \n",
    "    temp_df.loc[temp_df['theta'] > 350, 'theta'] = 0 #heuristic\n",
    "    temp_df.loc[temp_df['theta'] < 2, 'theta'] = 0 #heuristic\n",
    "    temp_df['quadrant'] = pd.cut(temp_df['theta'], 8, labels=range(1,9))\n",
    "    \n",
    "    #sort and retreive top-K, format required coordinates.\n",
    "    res_df = temp_df.sort_values(by=['Euclidean','theta'])[:k] #sort and get top-k neighbors\n",
    "    res_df = res_df.sort_values(by=['theta'])\n",
    "    res_df['tl_corner'] = res_df.apply(lambda row: (row['new_left'], row['new_top']), axis=1)\n",
    "    res_df['br_corner'] = res_df.apply(lambda row: (row['new_left']+row['new_width'], row['new_top']+row['new_height']), axis=1)           \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_ouptuts(img):\n",
    "    image, opening, thresh = binarize_img(img,return_inter=True)\n",
    "    totalLabels, labels, stats, centroid = cv2.connectedComponentsWithStats(thresh,4,cv2.CV_32S) # type: ignore #4 is for connectivity\n",
    "    df = generate_stats(totalLabels,stats, centroid)\n",
    "    df = filter_cv2_components(df)\n",
    "    output = image.copy() \n",
    "    for comp in df['label_id'].values:\n",
    "        x, y, w, h, area = df[df['label_id'] == comp][['left','top','width','height','area']].values[0]\n",
    "        cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 6))\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(\"image\")\n",
    "    axs[1].imshow(opening,cmap='gray')\n",
    "    axs[1].set_title(\"opening\")\n",
    "    axs[2].imshow(thresh,cmap='gray')\n",
    "    axs[2].set_title(\"thresh\")\n",
    "    axs[3].imshow(output)\n",
    "    axs[3].set_title(\"components\")\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.05, hspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_layout(img,k_neigh = 5):\n",
    "    '''\n",
    "    img : image, \n",
    "    k_neigh : #neighbors to build context\n",
    "    '''\n",
    "    #preprocess and transform image\n",
    "    thresh, img = binarize_img(img)\n",
    "    totalLabels, labels, stats, centroid = cv2.connectedComponentsWithStats(thresh,4,cv2.CV_32S)\n",
    "    df = generate_stats(totalLabels,stats, centroid)\n",
    "    df = filter_cv2_components(df)\n",
    "    ar_median = df['aspect_ratio'].median() #filter for smaller-words\n",
    "    \n",
    "    #extract context for each word component\n",
    "    context_vectors,context_coordinates = [],[]\n",
    "    for label in df.label_id.values:\n",
    "        try:\n",
    "            #if it's a small word, do not use it for indexing\n",
    "            # if(df[df['label_id'] == label]['aspect_ratio'].values[0] < ar_median): \n",
    "            #     continue\n",
    "            res = build_single_lexicon(label, df,k=k_neigh)\n",
    "            if(res.shape[0] >= k_neigh):\n",
    "                context_vectors.append(res[['tl_corner','br_corner']].values)\n",
    "                coords = df[df['label_id'] == label][['centroid_x','centroid_y']].values\n",
    "                context_coordinates.append(coords)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    # format into a set of 4/quadrapules\n",
    "    flattened_tuples = [np.array([item for sublist in t for item in sublist]) for t in context_vectors]\n",
    "    array_of_tuples = np.array(flattened_tuples)\n",
    "    context_vectors = array_of_tuples.reshape(len(context_vectors), k_neigh, 4) # 4 is for (TL-x,TL-y,BR-x,BR-y )\n",
    "    context_vectors = context_vectors.round(4)\n",
    "    #print(\"total no of contexts extracted:\",context_vectors.shape[0])\n",
    "    return context_vectors,context_coordinates, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDetails():\n",
    "    def __init__(self, img,feature_vectors,label_coordinates, img_path):\n",
    "        self.img = img\n",
    "        self.img_path = img_path\n",
    "        self.feature_vectors = feature_vectors \n",
    "        self.label_coordinates = label_coordinates #saves each word coordinates indexed wrt to feature_vectors\n",
    "        \n",
    "class DocumentVectors():\n",
    "    def __init__(self, img_folder_path):\n",
    "        \n",
    "        img_files = [file for file in os.listdir(img_folder_path) if file.endswith('.png')] #filter only image files\n",
    "        img_file_paths = [os.path.join(img_folder_path, file_name) for file_name in img_files] #generate filepaths for all images\n",
    "        \n",
    "        #initialize variables\n",
    "        self.img_file_paths = img_file_paths\n",
    "        self.docs = {} #save all document details\n",
    "        self.context_index = {} #save reduced context-vector-layouts index\n",
    "\n",
    "    #for all images, extract context layouts.\n",
    "    def extract_context_vectors(self, show_inter=0):\n",
    "        for id,img_path in enumerate(self.img_file_paths):\n",
    "            img = cv2.imread(img_path)\n",
    "            feature_vectors,label_coordinates, img = build_context_layout(img)\n",
    "            print(img_path,id, img.shape, feature_vectors.shape,len(label_coordinates))\n",
    "            self.docs[id] = DocumentDetails(img,feature_vectors,label_coordinates, img_path)\n",
    "            if show_inter:\n",
    "                plot_all_ouptuts(img)\n",
    "\n",
    "    \n",
    "    #build index from extracted context layouts\n",
    "    def build_context_index(self, ms=True, index_file_name='reduced_context_index2.npy'):\n",
    "        #if index already exists, load it. else build it and save.\n",
    "        if os.path.exists(index_file_name):\n",
    "            index = np.load(index_file_name,allow_pickle=True)\n",
    "            self.context_index = dict(index.flatten()[0])\n",
    "            print(\"loaded prebuilt index:\",index_file_name,\"in total:\",len(self.context_index))\n",
    "            return\n",
    "        #build a normal index with ctxlayout->docid pairs\n",
    "        index = {}\n",
    "        for id in self.docs:\n",
    "            cv_all = tuple(self.docs[id].feature_vectors)\n",
    "            for cv in cv_all:\n",
    "                index[tuple(list(cv.reshape(-1)))] = id\n",
    "        print(\"extracted all context vectors, in total: \",len(index))\n",
    "\n",
    "        if ms: #if ms is True, then perform Mean shift clustering to reduce index size\n",
    "            #reduce cl collection with Mean shift clustering\n",
    "            layouts = list(index.keys())\n",
    "            clusterer = MeanShift()\n",
    "            clusters = clusterer.fit(layouts) # type: ignore\n",
    "            cluster_labels = clusters.labels_\n",
    "            cluster_centers = clusters.cluster_centers_\n",
    "\n",
    "            #build new reduced ctxlayout-centre->[list of doc-ids] index\n",
    "            reduced_index = {}\n",
    "            for i in range(0,len(cluster_labels)):\n",
    "                cluster = cluster_labels[i]\n",
    "                centroid = tuple(cluster_centers[cluster])\n",
    "                doc = index[layouts[i]]\n",
    "                if centroid in reduced_index:\n",
    "                    reduced_index[centroid].add(doc)\n",
    "                else:\n",
    "                    reduced_index[centroid] = set([doc])\n",
    "\n",
    "            self.context_index = reduced_index\n",
    "            print(\"built mean-shift reduced-index, in total: \",len(reduced_index))\n",
    "        else:\n",
    "            self.context_index = index\n",
    "            print(\"built full-index, in total: \",len(index))\n",
    "        \n",
    "        np.save(index_file_name, self.context_index)\n",
    "        print(\"saved index, as \",index_file_name)\n",
    "\n",
    "    #display all documents in the database\n",
    "    def show_docs(self):\n",
    "        for id in self.docs:\n",
    "            img = self.docs[id].img\n",
    "            plot_all_ouptuts(img)     \n",
    "\n",
    "    #given query img, retreive relevant document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = './data/docs/'\n",
    "save_path = \"./dv_class.pickle\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    dv = pickle.load(open(save_path, \"rb\", -1))\n",
    "    print(\"loaded saved class from {}\".format(save_path))\n",
    "else:     \n",
    "    dv = DocumentVectors(docs_path)\n",
    "    dv.extract_context_vectors(show_inter=1)\n",
    "    dv.build_context_index()\n",
    "    with open(save_path, \"wb\") as file:\n",
    "        pickle.dump(dv, file, -1)\n",
    "    print(\"saved class object at {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv.show_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_quarter_crop(img):\n",
    "    img_width, img_height,_ = img.shape\n",
    "    max_left = img_width - img_width // 2\n",
    "    max_top = img_height - img_height // 2\n",
    "    left = random.randint(0, max_left)\n",
    "    top = random.randint(0, max_top)\n",
    "    crop_width = img_width // 2\n",
    "    crop_height = img_height // 2\n",
    "    cropped_img = img[left:left + crop_width, top:top + crop_height]\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = './data/docs/'\n",
    "img_id = 14\n",
    "img_path = docs_path + str(img_id) + '.png'\n",
    "print(img_path)\n",
    "img = cv2.imread(img_path)\n",
    "img_c = random_quarter_crop(img)\n",
    "plt.imshow(img_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(img_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_plot_matches2(q, d, m1, m2):\n",
    "        ''' \n",
    "        q: query image,\n",
    "        d: document image, \n",
    "        m1: matched point-set of query ,\n",
    "        m2: matched point-set of document,\n",
    "        '''\n",
    "        point_img = np.full((d.shape[0],d.shape[1]*2,3),255) #draw and image of 2* doc-image\n",
    "        point_img[:q.shape[0],:q.shape[1],:] = q #fill top-left with query\n",
    "        point_img[:d.shape[0],d.shape[1]:,:] = d #fill top-right with document\n",
    "        point_img = point_img.astype('uint8')\n",
    "\n",
    "        output_image = point_img.copy()\n",
    "        for match1, match2 in zip(m1, m2):\n",
    "            #difference between a match should be in range of (mean_centroid-threshold, mean_centroid+threshold)\n",
    "            (x1, y1) = match1\n",
    "            (x2, y2) = match2\n",
    "            x2 = x2+d.shape[1]\n",
    "            color = (0, 255, 0)\n",
    "            cv2.circle(output_image, (int(x1),int(y1)), radius=8, color=color, thickness=5)\n",
    "            cv2.circle(output_image, (int(x2),int(y2)), radius=8, color=color, thickness=5)\n",
    "            cv2.line(output_image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "        \n",
    "        plt.title('point-matches')\n",
    "        plt.imshow(output_image)\n",
    "        plt.show()\n",
    "\n",
    "def custom_plot_matches(q, d, m1, m2):\n",
    "        ''' \n",
    "        q: query image,\n",
    "        d: document image, \n",
    "        m1: matched point-set of query ,\n",
    "        m2: matched point-set of document,\n",
    "        '''\n",
    "        point_img = np.full((d.shape[0],d.shape[1]*2,3),255) #draw and image of 2* doc-image\n",
    "        point_img[:q.shape[0],:q.shape[1],:] = q #fill top-left with query\n",
    "        point_img[:d.shape[0],d.shape[1]:,:] = d #fill top-right with document\n",
    "        point_img = point_img.astype('uint8')\n",
    "\n",
    "        points = abs(m2-m1) #distribution of differences between matched coordinate pairs\n",
    "        mean_centroid = np.mean(points, axis=0) #mean centroid coordinate difference\n",
    "        threshold = np.std(points, axis=0) # threshold - std deviation of difference\n",
    "        \n",
    "        output_image = point_img.copy()\n",
    "        for match1, match2 in zip(m1, m2):\n",
    "            #difference between a match should be in range of (mean_centroid-threshold, mean_centroid+threshold)\n",
    "            diff = np.round(abs(match2-match1)) \n",
    "            c1 = np.all(diff > mean_centroid-threshold)\n",
    "            c2 = np.all(diff < mean_centroid+threshold)\n",
    "            (x1, y1) = match1\n",
    "            (x2, y2) = match2\n",
    "            x2 = x2+d.shape[1]\n",
    "            color = (0, 255, 0)\n",
    "            if(c1 and c2):\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                color = (255, 0, 0)\n",
    "            cv2.circle(output_image, (int(x1),int(y1)), radius=8, color=color, thickness=5)\n",
    "            cv2.circle(output_image, (int(x2),int(y2)), radius=8, color=color, thickness=5)\n",
    "            cv2.line(output_image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
    "\n",
    "        plt.title('point-matches')\n",
    "        plt.imshow(output_image)\n",
    "        plt.show()\n",
    "\n",
    "def point_match(query_img,doc_id,n_matches=30):\n",
    "        '''\n",
    "        query_img: query img,\n",
    "        id : doc-id\n",
    "        '''\n",
    "        #build context-layout for query_img\n",
    "        query_vectors,query_coordinates,img = build_context_layout(query_img)\n",
    "        doc_vectors,doc_coordinates,doc_img = [],[], None\n",
    "\n",
    "        #extract context-layout(actual, not reduced index) for doc=id\n",
    "        for id in dv.docs:\n",
    "            if(id == doc_id):\n",
    "                doc_vectors,doc_coordinates,doc_img = dv.docs[id].feature_vectors,dv.docs[id].label_coordinates,dv.docs[id].img\n",
    "                break\n",
    "\n",
    "        #for each query vector, find nearest match of context-vector,\n",
    "        #and then extract corresponding coordinate pairs from query & doc\n",
    "        matches1,matches2 = [],[]\n",
    "        for i in range(len(query_vectors)):\n",
    "            #get min-distance corresponding context layout\n",
    "            dist = np.linalg.norm(doc_vectors - query_vectors[i, np.newaxis, :, :], axis=(1, 2))\n",
    "            closest_index = np.argmin(dist)\n",
    "            query_coords = query_coordinates[i]\n",
    "            doc_coords = doc_coordinates[closest_index]\n",
    "            #append matches to return\n",
    "            matches1.append(query_coords[0])\n",
    "            matches2.append(doc_coords[0])\n",
    "            if len(matches1) >= n_matches:\n",
    "                break\n",
    "        # custom_plot_matches(query_img,doc_img,np.round(matches1,2),np.round(matches2,2))\n",
    "        custom_plot_matches(img,doc_img,np.round(matches1,2),np.round(matches2,2))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get orientation for a point set.\n",
    "def get_orientation(matrix):\n",
    "   '''\n",
    "   matrix : numpy 2d array\n",
    "   '''\n",
    "   determinant = np.linalg.det(matrix)\n",
    "   orientation = np.sign(determinant) \n",
    "   return orientation\n",
    "\n",
    "#match/verify orientation for 2 point sets.\n",
    "def verfiy_orientation(P1,P2):\n",
    "    '''\n",
    "    P1 : numpy 2d array,\n",
    "    P2 : numpy 2d array,\n",
    "    '''\n",
    "    s1 = get_orientation(P1)\n",
    "    s2 = get_orientation(P2)\n",
    "    verif = s1 * s2\n",
    "    return int(verif)\n",
    "\n",
    "#triplet-verification\n",
    "def verification(candidates, query_vectors, query_coordinates, m_matches=30, threshold_score=1000):\n",
    "        '''\n",
    "        candidates: candidate doc-id's,\n",
    "        query_coordinates: geometric coordinates of word components in query img,\n",
    "        m_matches : no of matched pairs to consider for  verification\n",
    "        '''\n",
    "        triplet_scores= {}\n",
    "        query_coordinates = np.array(query_coordinates).reshape(len(query_coordinates),2).round(2)\n",
    "        curr_score = -1\n",
    "        curr_matches = None\n",
    "        res_path = None\n",
    "        #for each candidate doc, build triplet score\n",
    "        for cd in candidates:  \n",
    "            dc,doc_vectors, path = dv.docs[cd].label_coordinates, dv.docs[cd].feature_vectors, dv.docs[cd].img_path\n",
    "            doc_coords = np.array(dc).reshape(len(dc),2).round(2)\n",
    "            doc_vectors = np.array(doc_vectors).reshape(len(doc_vectors),20).round(2)\n",
    "            #extract pair-wise distances for each cooridinate pair from query, candidate\n",
    "            distances = np.round(cdist(query_vectors, doc_vectors, metric='euclidean'),2) \n",
    "            #build a graph of cooridinate pair from query, candidate with edge-length as distance between them\n",
    "            edges_dict = {(tuple(query_coordinates[i]), tuple(doc_coords[j])): distances[i, j] for i in range(query_coordinates.shape[0]) for j in range(doc_coords.shape[0])}\n",
    "            #sort the graph by min edge lengths and extract the matched coordiante pairs from query, candidate\n",
    "            min_edges = sorted(edges_dict.items(), key=lambda x: x[1])\n",
    "            # print(distances.shape, query_vectors.shape,doc_vectors.shape )\n",
    "\n",
    "            #extract top m_matches of point-sets of matched items from query, candidate and verify for 1-1 correspondence\n",
    "            triplets,count = [],1\n",
    "            p1_set,p2_set = set(),set()\n",
    "            for edge in min_edges:\n",
    "                P1,P2,d = edge[0][0],edge[0][1], edge[1]\n",
    "                if(P1 not in p1_set) and (P2 not in p2_set):\n",
    "                    p1_set.add(P1)\n",
    "                    p2_set.add(P2)\n",
    "                    triplets.append((P1,P2))\n",
    "                    count += 1\n",
    "                if count >= m_matches:\n",
    "                    break\n",
    "            \n",
    "            #build triplet sets of top m_matches of pairs.\n",
    "            triplet_combinations = np.array(list(itertools.combinations(triplets, 3)))\n",
    "            #extract  matche scores for each triplet combination and sum up to make final score\n",
    "            matches_score = 0\n",
    "            #for each triplet coordinate-sets, verfiy orientation\n",
    "            for elm in triplet_combinations:\n",
    "                s1 = np.concatenate([elm[:,0,:], np.ones((elm[:,0,:].shape[0], 1))], axis=1) #2,3 => 3,3 matrix\n",
    "                s2 = np.concatenate([elm[:,1,:], np.ones((elm[:,1,:].shape[0], 1))], axis=1) #2,3 => 3,3 matrix\n",
    "                matches_score += verfiy_orientation(s1,s2) #\n",
    "            triplet_scores[cd] = matches_score # final triplet matches score for candidate cd\n",
    "            if(matches_score > curr_score):\n",
    "                 curr_score = matches_score\n",
    "                 curr_matches = triplets\n",
    "                 res_path = path\n",
    "\n",
    "\n",
    "        print(\"triplet verification scores:\",sorted(triplet_scores.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "        #sort by matches scores and return top-one.\n",
    "        results = sorted(triplet_scores, key=triplet_scores.get, reverse=True)\n",
    "        print(\"top triplet-score candidates:\", results)\n",
    "        if curr_score < threshold_score:\n",
    "            return -1,0,0\n",
    "        print(\"final retreived result document ID:\", results[0])\n",
    "        return results[0], curr_matches, res_path\n",
    "    \n",
    "    #verify 1-1 point match between a query img and a given doc-id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query_img, max_candidates=10,min_qry_vectors=10):\n",
    "        '''\n",
    "        query_img: query img,\n",
    "        max_candidates: no of candidates for triplet verification\n",
    "        '''\n",
    "        #get query-img context layouts\n",
    "        query_vectors,query_coordinates,img = build_context_layout(query_img)\n",
    "        if len(query_vectors)<min_qry_vectors: #return if <10 query vectors found\n",
    "            print(\"found only\",len(query_vectors),\"query vectors. cannot query with this img. try with a better image\")\n",
    "            return -1\n",
    "        print(\"found\",len(query_vectors),\"query vectors\")\n",
    "        query_vectors = query_vectors.reshape(len(query_vectors),20)\n",
    "\n",
    "        #get matchings-count score\n",
    "        index = np.array(list(dv.context_index.keys()))\n",
    "        docs = np.array(list(dv.context_index.values()))\n",
    "        coverage_scores = {}\n",
    "        distances = cdist(query_vectors, index) #extract distances between each pair of index-vectors,query-vectors \n",
    "        best_match_indices = np.argmin(distances, axis=1) #filter best matches of pairs\n",
    "        print(best_match_indices,query_vectors.shape, index.shape)\n",
    "        for bm in best_match_indices: #build coverage scores(no of matched index-vectors) for each document.\n",
    "            for elm in set(docs[bm]):\n",
    "                if elm in coverage_scores:\n",
    "                    coverage_scores[elm] += 1\n",
    "                else:\n",
    "                    coverage_scores[elm] = 0\n",
    "        print(\"layout coverage scores:\",sorted(coverage_scores.items(), key=lambda x:x[1],reverse=True))\n",
    "\n",
    "        #sort by matchings-count/coverage scores & filter top max_candidates\n",
    "        candidates = sorted(coverage_scores, key=coverage_scores.get, reverse=True)#[:max_candidates]\n",
    "        print(\"top layout-match candidates : \", candidates)\n",
    "        doc_id, matches, doc_path = verification(candidates, query_vectors, query_coordinates)\n",
    "        if doc_id == -1:\n",
    "            print(\"looks like the document is not present in the database\")\n",
    "            return -1\n",
    "        matches = np.array(matches)\n",
    "        m1 = matches[:,0,:]\n",
    "        m2 = matches[:,1,:]\n",
    "        print(doc_path)\n",
    "        \n",
    "        # point_match(query_img, doc_id)\n",
    "        doc_img = dv.docs[doc_id].img\n",
    "        custom_plot_matches2(img,doc_img,np.round(m1,2),np.round(m2,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", img_id)\n",
    "query(img_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_match(img_c, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = cv2.imread(\"./data/queries/2.jpeg\")\n",
    "plt.imshow(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", 1)\n",
    "query(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_match(q2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q12 = cv2.imread(\"./data/queries/2_CS.jpeg\")\n",
    "plt.imshow(q12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(q12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", 1)\n",
    "query(q12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_match(q12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ROI(image):\n",
    "    \n",
    "    gray = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Remove horizontal lines\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25,1))\n",
    "    detected_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=1)\n",
    "    cnts = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    for c in cnts:\n",
    "        cv2.drawContours(thresh, [c], -1, 0, -1)\n",
    "\n",
    "    # Dilate to merge into a single contour\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,30))\n",
    "    dilate = cv2.dilate(thresh, vertical_kernel, iterations=3)\n",
    "\n",
    "    # Find contours, sort for largest contour and extract ROI\n",
    "    cnts, _ = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:-1]\n",
    "    c = cnts[0]\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    # cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 4)\n",
    "    ROI = image[y:y+h, x:x+w]\n",
    "    return ROI\n",
    "# plt.imshow(extract_ROI(q4))\n",
    "\n",
    "def deskew(image):\n",
    "    grayscale = rgb2gray(image)\n",
    "    angle = determine_skew(grayscale)\n",
    "    print(\"angle:\", angle)\n",
    "    rotated = (rotate(image, angle, resize=True) * 255).astype(np.uint8)\n",
    "    return rotated\n",
    "# plt.imshow(deskew(extract_ROI(q4)))\n",
    "\n",
    "def normalize_image(image, target_width=600, target_height=800):\n",
    "    original_height, original_width = image.shape[:2]\n",
    "    scale_x = target_width / original_width\n",
    "    scale_y = target_height / original_height\n",
    "    scaled_image = cv2.resize(image, (target_width, target_height))\n",
    "    return scaled_image\n",
    "# plt.imshow(normalize_image(q4))\n",
    "\n",
    "def preprocess2(img):\n",
    "    img = extract_ROI(img)\n",
    "    img = deskew(img)\n",
    "    img= normalize_image(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = cv2.imread('./data/queries/cam_cap.jpeg')\n",
    "plt.imshow(q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", 10)\n",
    "query(q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q33 = preprocess2(q3)\n",
    "plt.imshow(q33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(q33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", 10)\n",
    "query(q33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_44 = cv2.imread(\".//data/queries/query_31.png\")\n",
    "plt.imshow(query_44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(query_44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", 31)\n",
    "query(query_44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q44 = preprocess2(query_44)\n",
    "plt.imshow(q44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(q44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", 31)\n",
    "query(q44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_5 = cv2.imread(\"./data/queries/d1.jpg\")\n",
    "plt.imshow(query_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(query_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", None)\n",
    "query(query_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q55 = preprocess2(query_5)\n",
    "plt.imshow(q55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ouptuts(q55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ground truth: \", None)\n",
    "query(q55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
